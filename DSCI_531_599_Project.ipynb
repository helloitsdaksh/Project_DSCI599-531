{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "11H3g0lUyczhSK4i-ObYzPPbBqAU2LXHd",
      "authorship_tag": "ABX9TyPgIKT6yCT6omkaT4XuYdod",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helloitsdaksh/Project_DSCI599-531/blob/main/DSCI_531_599_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyFxI7tUkmW-",
        "outputId": "dcc63955-3e58-439f-f983-e9f7c33237f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDQknVA-kcQf"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import TargetEncoder\n",
        "import optuna\n",
        "import os\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEMALE_POP = 0.536\n",
        "FEMALE_CAMPAIGN_POP = 0.392619\n",
        "MALE_POP = 0.464\n",
        "MALE_CAMPAIGN_POP = 0.607381\n",
        "FEMALE_RATIO = FEMALE_POP / FEMALE_CAMPAIGN_POP\n",
        "MALE_RATIO = MALE_POP / MALE_CAMPAIGN_POP"
      ],
      "metadata": {
        "id": "PgXYVVtfkhkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZa4pz7Vl63a",
        "outputId": "df7291f0-5da0-44c5-c003-377db1350b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------\n",
        "# Data utilities\n",
        "--------------------------------------------------------------"
      ],
      "metadata": {
        "id": "w_wLgJyWxBDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss, roc_auc_score, average_precision_score\n",
        "\n",
        "def load_data(\n",
        "\t\tnrows: int = None,\n",
        "\t\tfilename: str = \"fairjob.csv.gz\"\n",
        "\t\t):\n",
        "\t\"\"\"\n",
        "\tData loader for FairJob dataset.\n",
        "\n",
        "\tArgs:\n",
        "\t\tnrows (int): How many rows to load for np.loadtxt. Defaults to None.\n",
        "\t\tfilename (str): Name of data file located in data/. Defaults to \"fairjob.csv.gz\".\n",
        "\n",
        "\tReturns:\n",
        "\t\tX (numpy.ndarray): (n_data, n_features)\n",
        "\t\tclick (np.ndarray): (n_data,)\n",
        "\t\tprotected_attribute (numpy.ndarray): (n_data,)\n",
        "\t\tsenior (numpy.ndarray): (n_data,)\n",
        "\t\tdisplayrandom (numpy.ndarray): (n_data,)\n",
        "\t\trank (numpy.ndarray): (n_data,)\n",
        "\t\tcategorical_features_cardinalities (dict): categorical features cardinalities\n",
        "\t\"\"\"\n",
        "\n",
        "\tdata = np.loadtxt((filename), skiprows=1, delimiter=\",\", max_rows=nrows)\n",
        "\tclick_idx = 0\n",
        "\tprotected_attribute_idx = 1\n",
        "\tsenior_idx = 2\n",
        "\tdisplayrandom_idx = 3\n",
        "\trank_idx = 4\n",
        "\tX = data[:, 5:]\n",
        "\tn_cat_cols = np.sum(X.mean(axis=0) > 1e-1)\n",
        "\tcategorical_features_idx = np.arange(0, n_cat_cols)\n",
        "\tcategorical_features_cardinalities = dict()\n",
        "\n",
        "\t# counting unique tokens per categorical variable\n",
        "\t# and renumbering them from 0\n",
        "\tfor dim in range(len(categorical_features_idx)):\n",
        "\t\trenumber_dict = dict()\n",
        "\t\tvalues = np.unique(X[:, dim])\n",
        "\t\tcategorical_features_cardinalities[dim] = len(values)\n",
        "\t\tfor i, v in enumerate(values):\n",
        "\t\t\trenumber_dict[v] = i\n",
        "\t\tfor i in range(len(X)):\n",
        "\t\t\tX[i, dim] = renumber_dict[X[i, dim]]\n",
        "\n",
        "\treturn (\n",
        "\t\tX,\n",
        "\t\tdata[:, click_idx],\n",
        "\t\tdata[:, protected_attribute_idx],\n",
        "\t\tdata[:, senior_idx],\n",
        "\t\tdata[:, displayrandom_idx],\n",
        "\t\tdata[:, rank_idx],\n",
        "\t\tcategorical_features_cardinalities,\n",
        "\t\t)\n",
        "\n",
        "class JobDataset(Dataset):\n",
        "\t\"\"\"\n",
        "\tClass collecting the variables of the data necessary for batch generation.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, X, y, protected_attribute, fair_indicator):\n",
        "\t\tself.X = X\n",
        "\t\tself.y = y\n",
        "\t\tself.protected_attribute = protected_attribute\n",
        "\t\tif fair_indicator is None:\n",
        "\t\t\tself.fair_indicator = torch.ones_like(y).to(torch.int).to(y.device)\n",
        "\t\telse:\n",
        "\t\t\tself.fair_indicator = fair_indicator\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.y)\n",
        "\n",
        "\tdef get_target(self):\n",
        "\t\treturn self.y\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.X[idx], self.y[idx], self.protected_attribute[idx], self.fair_indicator[idx]\n",
        "\n",
        "def batch_loader(dataset: JobDataset, batch_size: int):\n",
        "\t\"\"\"\n",
        "\tFactory for batch data loader with weighted random sampling for addressing class imbalance.\n",
        "\n",
        "\tArgs:\n",
        "\t\tdataset (JobDataset): Training data\n",
        "\t\tbatch_size (int): Batch size\n",
        "\n",
        "\tReturns:\n",
        "\t\tDataLoader\n",
        "\t\"\"\"\n",
        "\n",
        "\tclass_counts = torch.bincount(dataset.get_target())\n",
        "\tclass_weights = 1.0 / class_counts.float()\n",
        "\t# Create weights for each sample in the dataset\n",
        "\tsample_weights = class_weights[dataset.get_target()]\n",
        "\tsample_weights = sample_weights.detach().cpu().numpy()\n",
        "\n",
        "\tsampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\treturn DataLoader(dataset=dataset, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "\n",
        "def train_test_split(\n",
        "\t\tX, y, protected_attribute, is_senior, displayrandom, rank, train_fraction: float = 0.8\n",
        "\t\t):\n",
        "\t\"\"\"\n",
        "\tRandom split of data in training and test sets.\n",
        "\n",
        "\tArgs:\n",
        "\t\tX: Features\n",
        "\t\ty: Target labels\n",
        "\t\tprotected_attribute: Protected attribute\n",
        "\t\tis_senior: Flag for senior ads.\n",
        "\t\tdisplayrandom: Flag for ads with random display order in the banner.\n",
        "\t\trank: Ads rank in the banner.\n",
        "\t\ttrain_fraction (float): Fraction for training data. Defaults to 0.8.\n",
        "\n",
        "\tReturns:\n",
        "\t\tX_train,\n",
        "\t\tX_test,\n",
        "\t\ty_train,\n",
        "\t\ty_test,\n",
        "\t\tprotected_attribute_train,\n",
        "\t\tprotected_attribute_test,\n",
        "\t\tis_senior_train,\n",
        "\t\tis_senior_test,\n",
        "\t\tdisplayrandom_train,\n",
        "\t\tdisplayrandom_test\n",
        "\t\trank_train,\n",
        "\t\trank_test\n",
        "\t\"\"\"\n",
        "\n",
        "\tcut = int(len(X) * train_fraction * 100 // 100)\n",
        "\tX_train = X[:cut, :]\n",
        "\tX_test = X[cut:, :]\n",
        "\ty_train = y[:cut]\n",
        "\ty_test = y[cut:]\n",
        "\tprotected_attribute_train = protected_attribute[:cut]\n",
        "\tprotected_attribute_test = protected_attribute[cut:]\n",
        "\tis_senior_train = is_senior[:cut]\n",
        "\tis_senior_test = is_senior[cut:]\n",
        "\tdisplayrandom_train = displayrandom[:cut]\n",
        "\tdisplayrandom_test = displayrandom[cut:]\n",
        "\trank_train = rank[:cut]\n",
        "\trank_test = rank[cut:]\n",
        "\treturn (\n",
        "\t\tX_train,\n",
        "\t\tX_test,\n",
        "\t\ty_train,\n",
        "\t\ty_test,\n",
        "\t\tprotected_attribute_train,\n",
        "\t\tprotected_attribute_test,\n",
        "\t\tis_senior_train,\n",
        "\t\tis_senior_test,\n",
        "\t\tdisplayrandom_train,\n",
        "\t\tdisplayrandom_test,\n",
        "\t\trank_train,\n",
        "\t\trank_test,\n",
        "\t\t)"
      ],
      "metadata": {
        "id": "jFlm0h8MkskR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------\n",
        "# Models\n",
        "--------------------------------------------------------------"
      ],
      "metadata": {
        "id": "fa8wi-igxia1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyClassifier(nn.Module):\n",
        "\t\"\"\"\n",
        "\tClassifier based on single threshold for positive class probability.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, start_p: float = 0.005):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.bias = nn.Parameter(Tensor([1 - start_p, start_p]))\n",
        "\t\tself.bias.requires_grad = True\n",
        "\t\tself.register_buffer(\"zero_const\", torch.zeros((1, 2)))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.zero_const.repeat(x.shape[0], 1) + self.bias\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn \"Dummy\"\n",
        "\n",
        "\n",
        "class MixedEmbedding(nn.Module):\n",
        "\t\"\"\"\n",
        "\tLayer for embedding categorical and continuous features for Logistic Regression.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\tinput_dim: int,\n",
        "\t\t\tcategorical_features_cardinalities: dict,\n",
        "\t\t\tembedding_size: int = 10,\n",
        "\t\t\t):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tinput_dim (int): dimension of input features\n",
        "\t\t\tcategorical_features_cardinalities (dict): cardinalities (values) for each categorical feature (column index in the data as key)\n",
        "\t\t\tembedding_size (int): Output size of the mixed embedding layer. Defaults to 10.\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\tself.embedding_size = embedding_size\n",
        "\t\tself.categorical_features_idx = np.array(\n",
        "\t\t\t[_ for _ in categorical_features_cardinalities.keys()]\n",
        "\t\t\t)\n",
        "\t\tself.numerical_features_idx = np.array(\n",
        "\t\t\t[_ for _ in range(input_dim) if _ not in self.categorical_features_idx]\n",
        "\t\t\t)\n",
        "\t\tself.embeddings = nn.ModuleList(\n",
        "\t\t\t[\n",
        "\t\t\t\tnn.Embedding(\n",
        "\t\t\t\t\tnum_embeddings=categorical_features_cardinalities[cat_feature_idx],\n",
        "\t\t\t\t\tembedding_dim=min(\n",
        "\t\t\t\t\t\tembedding_size,\n",
        "\t\t\t\t\t\tcategorical_features_cardinalities[cat_feature_idx],\n",
        "\t\t\t\t\t\t),\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\tfor cat_feature_idx in self.categorical_features_idx\n",
        "\t\t\t\t]\n",
        "\t\t\t)\n",
        "\t\tself.output_dimension = len(self.numerical_features_idx) + sum(\n",
        "\t\t\t[e.embedding_dim for e in self.embeddings]\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tcatx = x[:, self.categorical_features_idx].int()\n",
        "\t\tnumx = x[:, self.numerical_features_idx]\n",
        "\t\tembeddedx = torch.hstack(\n",
        "\t\t\t[self.embeddings[_](catx[:, _]) for _ in self.categorical_features_idx]\n",
        "\t\t\t)\n",
        "\t\tembeddedx = embeddedx.view(-1, self.output_dimension - len(self.numerical_features_idx))\n",
        "\t\treturn torch.hstack([embeddedx, numx])\n",
        "\n",
        "\tdef to(self, device):\n",
        "\t\tself.embeddings.to(device)\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "\t\"\"\"\n",
        "\tLogistic regression classifier.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\tinput_dim: int,\n",
        "\t\t\tcategorical_features_cardinalities: dict,\n",
        "\t\t\tembedding_size: int,\n",
        "\t\t\t):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tinput_dim (int): dimension of input features\n",
        "\t\t\tcategorical_features_cardinalities (dict): cardinalities (values) for each categorical feature (column index in the data as key)\n",
        "\t\t\tembedding_size (int): Output size of the mixed embedding layer.\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.mixed_embedding_layer = MixedEmbedding(\n",
        "\t\t\tinput_dim,\n",
        "\t\t\tcategorical_features_cardinalities,\n",
        "\t\t\tembedding_size=embedding_size,\n",
        "\t\t\t)\n",
        "\t\tself.weights = nn.Linear(\n",
        "\t\t\tin_features=self.mixed_embedding_layer.output_dimension, out_features=2\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\txx = self.mixed_embedding_layer(x)\n",
        "\t\treturn self.weights(xx)\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn \"LR\"\n",
        "\n",
        "\tdef to(self, device):\n",
        "\t\tself.mixed_embedding_layer.to(device)\n",
        "\t\tself.weights.to(device)"
      ],
      "metadata": {
        "id": "-vTkVEdlxztm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# Learning routines\n",
        "--------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "jaish22oyKg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def l2_conditional_independence_penalty(\n",
        "\t\ty_hat: Tensor, y_true: Tensor, protected_attribute: Tensor, fair_indicator: Tensor, *kwars\n",
        "\t\t):\n",
        "\t\"\"\"\n",
        "\tImplementation of the fairness penalty based on Bechavod & Ligett - https://arxiv.org/abs/1707.00044.\n",
        "\tDifferently from the original proposal, it uses the squared discrepancies between\n",
        "\tthe unconditional values of FPR, FNR, TPR, TNR and their conditional values, for both protected attribute labels.\n",
        "\n",
        "\tArgs:\n",
        "\t\ty_hat (Tensor): Predicted probabilities for the two target labels. Dimensions: (n_data, 2)\n",
        "\t\ty_true (Tensor): True labels. Dimensions: (n_data,)\n",
        "\t\tprotected_attribute (Tensor): Protected attributed values. Dimensions: (n_data,)\n",
        "\t\tfair_indicator (Tensor): Indicator for observations to be included in the penalty computation. Dimensions: (n_data,)\n",
        "\n",
        "\tReturns:\n",
        "\t\tValue of the fairness penalty\n",
        "\t\"\"\"\n",
        "\n",
        "\tassert y_hat.shape[1] == 2\n",
        "\tsum_of_squares = Tensor([0.0]).to(y_hat.device)\n",
        "\n",
        "\ty_hat = y_hat[fair_indicator, :]\n",
        "\ty_true = y_true[fair_indicator]\n",
        "\tprotected_attribute = protected_attribute[fair_indicator]\n",
        "\tfor y_hat_dim in (0, 1):\n",
        "\t\ty_hat_col = y_hat[:, y_hat_dim]\n",
        "\t\tfor y_dim in (0, 1):\n",
        "\t\t\tif torch.sum(y_true == y_dim) == 0.0:  # no such label\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ty_hat_avg = torch.mean(y_hat_col[y_true == y_dim])\n",
        "\t\t\tfor a_dim in (0, 1):\n",
        "\t\t\t\tif (\n",
        "\t\t\t\t\t\ttorch.sum((y_true == y_dim) & (protected_attribute == a_dim)) == 0.0\n",
        "\t\t\t\t):  # no such label given attribute\n",
        "\t\t\t\t\ty_hat_cond_a_avg = torch.tensor(0.0)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ty_hat_cond_a_avg = torch.mean(\n",
        "\t\t\t\t\t\ty_hat_col[(y_true == y_dim) & (protected_attribute == a_dim)]\n",
        "\t\t\t\t\t\t)\n",
        "\t\t\t\tsum_of_squares += (y_hat_avg - y_hat_cond_a_avg) ** 2\n",
        "\tpenalty = torch.sqrt(sum_of_squares)[0]\n",
        "\treturn penalty\n",
        "\n",
        "\n",
        "class Learner(object):\n",
        "\t\"\"\"\n",
        "\tGeneral class for learner.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\tbase_model,\n",
        "\t\t\tdevice=\"cuda\",\n",
        "\t\t\tbasename: str = \"REGULAR\",\n",
        "\t\t\tscheduler_step_size=30,\n",
        "\t\t\tscheduler_gamma=0.1,\n",
        "\t\t\t**optimizer_options\n",
        "\t\t\t):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tbase_model: Base model class  (e.g., Dummy, LogisticRegression)\n",
        "\t\t\tdevice (str): Device for PyTorch. Defaults to \"cuda\".\n",
        "\t\t\tbasename (str): Model name. Defaults to \"REGULAR\".\n",
        "\t\t\tscheduler_step_size (int): Step size parameter for learning rate scheduler. Defaults to 30.\n",
        "\t\t\tscheduler_gamma (float): Gamma parameter for learning rate scheduler. Defaults to 0.1.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.model = base_model\n",
        "\t\tself.model.ps_init = nn.Parameter(torch.randn(1), requires_grad=False)\n",
        "\t\tself.model.ps = nn.Parameter(self.model.ps_init.data, requires_grad=True)\n",
        "\t\tself.basename = basename\n",
        "\t\tself.loss = nn.CrossEntropyLoss()\n",
        "\t\tself.optimizer = torch.optim.Adam(self.model.parameters(), **optimizer_options)\n",
        "\t\tself.scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "\t\t\tself.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma\n",
        "\t\t\t)\n",
        "\t\tself.device = device\n",
        "\t\tself.model.to(device)\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn self.basename + \" \" + str(self.model).split(\"(\")[0]\n",
        "\n",
        "\tdef fit(\n",
        "\t\t\tself,\n",
        "\t\t\tx: Tensor,\n",
        "\t\t\ty: Tensor,\n",
        "\t\t\ta: Tensor,\n",
        "\t\t\tbatch_size=1024,\n",
        "\t\t\tpenalty_fun=None,\n",
        "\t\t\tpenalty_multiplier: float = 0.1,\n",
        "\t\t\tfair_indicator: Tensor = None,\n",
        "\t\t\t):\n",
        "\t\t\"\"\"\n",
        "\t\tFit function (single epoch).\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\tx (Tensor): Training features\n",
        "\t\t\ty (Tensor): Training target labels\n",
        "\t\t\ta (Tensor): Training protected attribute\n",
        "\t\t\tbatch_size (int): Batch size. Defaults to 1024.\n",
        "\t\t\tpenalty_fun : Additional penalty function to include fairness penalty. Defaults to None.\n",
        "\t\t\tpenalty_multiplier (float): Multiplier for additional penalty function. Defaults to 0.1.\n",
        "\t\t\tfair_indicator (Tensor):  Indicator for observations to be included in the penalty computation. Defaults to None.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tLoss value\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tloss_value = 0\n",
        "\n",
        "\t\tdata_train = JobDataset(x, y, a, fair_indicator)\n",
        "\t\tfor batch in batch_loader(data_train, batch_size=batch_size):\n",
        "\t\t\tx_b, y_b, a_b, fair_indicator_b = batch\n",
        "\t\t\to_b = x_b[:, 1]\n",
        "\t\t\tself.optimizer.zero_grad()\n",
        "\t\t\toutputs_b = self.model(x_b)\n",
        "\n",
        "\t\t\tloss_b = self.loss(outputs_b, y_b)\n",
        "\n",
        "\t\t\tif penalty_fun is not None:\n",
        "\t\t\t\tprobas_b = torch.softmax(outputs_b, dim=1)\n",
        "\t\t\t\tassert torch.max(probas_b) <= 1.0, torch.max(probas_b)\n",
        "\t\t\t\tassert torch.min(probas_b) >= 0.0, torch.min(probas_b)\n",
        "\t\t\t\tloss_b += penalty_multiplier * penalty_fun(\n",
        "\t\t\t\t\ty_hat=probas_b,\n",
        "\t\t\t\t\ty_true=y_b,\n",
        "\t\t\t\t\tprotected_attribute=a_b,\n",
        "\t\t\t\t\tfair_indicator=fair_indicator_b,\n",
        "\t\t\t\t\t)\n",
        "\n",
        "\t\t\tloss_b.backward()\n",
        "\t\t\tloss_value += loss_b.detach().cpu().numpy()\n",
        "\t\t\tself.optimizer.step()\n",
        "\n",
        "\t\treturn loss_value\n",
        "\n",
        "\tdef scheduler_step(self):\n",
        "\t\tself.scheduler.step()\n",
        "\n",
        "\tdef __call__(self, *args, **kwargs):\n",
        "\t\treturn self.model(*args, **kwargs)\n",
        "\n",
        "\tdef get_ps(self, *args, **kwargs):\n",
        "\t\treturn self.model.ps.data\n",
        "\n",
        "\tdef get_ps_init(self, *args, **kwargs):\n",
        "\t\treturn self.model.ps_init.data\n",
        "\n",
        "\tdef to(self, device: str):\n",
        "\t\tself.device = device\n",
        "\t\tself.model.to(device)\n",
        "\t\tself.loss.to(device)"
      ],
      "metadata": {
        "id": "s85aPFFNyOs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "--------------------------------------------------------------------------------\n",
        "# Metrics\n",
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "rsQBLTRVyjqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def demographic_parity(predictions, protected_attribute, scope):\n",
        "\t\"\"\"\n",
        "\tFunction to compute demographic parity with respect to scope.\n",
        "\n",
        "\tArgs:\n",
        "\t\tpredictions: model predictions\n",
        "\t\tprotected_attribute: protected attribute values\n",
        "\t\tscope: indicator for scope\n",
        "\n",
        "\tReturns:\n",
        "\t   Demographic parity\n",
        "\t\"\"\"\n",
        "\n",
        "\tres = 0.0\n",
        "\tfor col in (0, 1):\n",
        "\t\tres += torch.abs(\n",
        "\t\t\ttorch.mean(predictions[(protected_attribute > 0) & (scope > 0)][:, col])\n",
        "\t\t\t- torch.mean(predictions[(protected_attribute <= 0) & (scope > 0)][:, col])\n",
        "\t\t\t)\n",
        "\treturn res / 2.0\n",
        "\n",
        "\n",
        "def utility(\n",
        "\t\ty_pred: Tensor,\n",
        "\t\ty: Tensor,\n",
        "\t\tprotected_attribute: Tensor,\n",
        "\t\timpressions: Tensor,\n",
        "\t\tdisplayrandom: Tensor,\n",
        "\t\t):\n",
        "\t\"\"\"\n",
        "\tClick utility function.\n",
        "\n",
        "\tArgs:\n",
        "\t\ty_pred (Tensor): Predicted probabilities for the two target labels. Dimensions (n_data, 2)\n",
        "\t\ty (Tensor): Target labels. Dimensions (n_data,)\n",
        "\t\tprotected_attribute (Tensor): Protected attribute. Dimensions (n_data,)\n",
        "\t\timpressions (Tensor): Impression_id. Dimensions (n_data,)\n",
        "\t\tdisplayrandom (Tensor): Flag for ads with random display order in the banner. Dimensions (n_data,)\n",
        "\n",
        "\tReturns:\n",
        "\t\tValue of the utility\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Higher is better\n",
        "\tres = 0.0\n",
        "\tclick_col = 1\n",
        "\t# Subselect only data with randomized display\n",
        "\tmask = (displayrandom > 0)\n",
        "\ty_pred = y_pred[mask]\n",
        "\ty = y[mask]\n",
        "\tprotected_attribute = protected_attribute[mask]\n",
        "\timpressions = impressions[mask]\n",
        "\n",
        "\tfor impression in torch.unique(impressions):\n",
        "\t\ty_ranked = (torch.argsort(y_pred[impressions == impression,click_col]) + 1).to(torch.float)\n",
        "\t\ty_clicked = y[impressions == impression]\n",
        "\t\tclick_rank_impression = torch.mean(y_ranked * y_clicked)\n",
        "\n",
        "\t\tres += click_rank_impression\n",
        "\n",
        "\treturn res / (torch.unique(impressions)).size(dim=0)\n",
        "\n",
        "\n",
        "def utility_product(\n",
        "\t\ty_pred, y, protected_attribute, impressions, displayrandom, product, unbiased_ratio=False\n",
        "\t\t):\n",
        "\t\"\"\"\n",
        "\tProduct utility function.\n",
        "\n",
        "\tArgs:\n",
        "\t\ty_pred (Tensor): Predicted probabilities for the two target labels. Dimensions (n_data, 2)\n",
        "\t\ty (Tensor): Target labels. Dimensions (n_data,)\n",
        "\t\tprotected_attribute (Tensor): Protected attribute. Dimensions (n_data,)\n",
        "\t\timpressions (Tensor): Impression_id. Dimensions (n_data,)\n",
        "\t\tdisplayrandom (Tensor): Flag for ads with random display order in the banner. Dimensions (n_data,)\n",
        "\t\tunbiased_ratio (bool): True if the utility should be corrected with the data-population ratio of the protected attribute. Defaults to False.\n",
        "\n",
        "\tReturns:\n",
        "\t\tValue of the utility\n",
        "\t\"\"\"\n",
        "\n",
        "\tres = 0.0\n",
        "\tclick_col = 1\n",
        "\t# Subselect only data with randomized display\n",
        "\tmask = (displayrandom > 0)\n",
        "\ty_pred = y_pred[mask]\n",
        "\ty = y[mask]\n",
        "\tprotected_attribute = protected_attribute[mask]\n",
        "\timpressions = impressions[mask]\n",
        "\tproduct = product[mask]\n",
        "\n",
        "\t# Create rank probs for each impression\n",
        "\ty_ranked = torch.zeros_like(y,dtype=torch.float)\n",
        "\tfor impression in torch.unique(impressions):\n",
        "\t\ty_ranked[impressions == impression] = (torch.argsort(y_pred[impressions == impression,click_col]) + 1).to(torch.float)\n",
        "\n",
        "\tif unbiased_ratio:\n",
        "\t\tratio = (protected_attribute == 0) * FEMALE_RATIO + \\\n",
        "\t\t\t\t(protected_attribute == 1) * MALE_RATIO\n",
        "\telse:\n",
        "\t\tratio = torch.ones_like(protected_attribute, dtype=torch.float)\n",
        "\n",
        "\tfor prod in torch.unique(product):\n",
        "\t\tprod_mask = (product == prod)\n",
        "\t\tfor impression in torch.unique(impressions[prod_mask]):\n",
        "\t\t\tsize_impressions_prod = torch.unique(impressions[prod_mask]).shape[0]\n",
        "\t\t\timpression_prod_mask = (impressions == impression) & prod_mask # Not really necessary, each impression is for a specific product\n",
        "\t\t\ty_ranked_prod = y_ranked[impression_prod_mask]\n",
        "\t\t\ty_clicked_prod = y[impression_prod_mask]\n",
        "\n",
        "\t\t\tclick_rank_impression = (y_ranked_prod * ratio[impression_prod_mask]).dot(y_clicked_prod.to(torch.float))\n",
        "\t\t\tres += click_rank_impression/size_impressions_prod\n",
        "\n",
        "\treturn res/ torch.unique(product).shape[0]\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "\t\tres_pred_df: pd.DataFrame,\n",
        "\t\tresults_df: pd.DataFrame,\n",
        "\t\tname: str,\n",
        "\t\tl2_fair: float,\n",
        "\t\tfair_frac: float,\n",
        "\t\tsim: int,\n",
        "\t\tmodel,\n",
        "\t\tX_test: Tensor,\n",
        "\t\ty_test: Tensor,\n",
        "\t\tprotected_attribute_test: Tensor,\n",
        "\t\tis_senior_test: Tensor,\n",
        "\t\timpression_test: Tensor,\n",
        "\t\tdisplayrandom_test: Tensor,\n",
        "\t\tproduct_test: Tensor,\n",
        "\t\t):\n",
        "\t\"\"\"\n",
        "\tFunction for computing model evaluation metrics and save results.\n",
        "\n",
        "\tArgs:\n",
        "\t\tres_pred_df (pd.DataFrame): DataFrame for saving model predictions.\n",
        "\t\tresults_df (pd.DataFrame): DataFrame for saving model metrics.\n",
        "\t\tname (str): Model name as used in DataFrame index.\n",
        "\t\tl2_fair (float): Fairness penalty multiplier. Use None if without penalty.\n",
        "\t\tfair_frac (float): Fraction of data used to compute the fairness penalty. Use None if without penalty.\n",
        "\t\tsim (int): Simulation index.\n",
        "\t\tmodel: Model to use for predictions.\n",
        "\t\tX_test (Tensor): Test features.\n",
        "\t\ty_test (Tensor): Test target labels.\n",
        "\t\tprotected_attribute_test (Tensor): Test protected attribute.\n",
        "\t\tis_senior_test (Tensor): Test senior ads indicator.\n",
        "\t\timpression_test (Tensor): Test impression id.\n",
        "\t\tdisplayrandom_test (Tensor): Test displayrandom.\n",
        "\t\tproduct_test (Tensor): Test product id.\n",
        "\n",
        "\tReturns:\n",
        "\t\tres_pred_df: updated with new values. Note that results_df is modified in-place.\n",
        "\t\"\"\"\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\ty_pred = model(X_test)\n",
        "\t\tp = torch.softmax(y_pred, dim=1)\n",
        "\tif l2_fair is not None:\n",
        "\t\t# We save fair models\n",
        "\t\trows_index = (name,l2_fair,fair_frac,sim)\n",
        "\telse:\n",
        "\t\t# We save regular models\n",
        "\t\trows_index = (name,sim)\n",
        "\n",
        "\t# Saving prediction\n",
        "\tres_pred_df = pd.concat(\n",
        "\t\t[\n",
        "\t\t\tres_pred_df,\n",
        "\t\t\tpd.DataFrame(\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"prob_test\": p[:,1].detach().cpu().numpy(),\n",
        "\t\t\t\t\t\"y_test\": y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\"a_test\": protected_attribute_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\"s_test\": is_senior_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\"displayrandom_test\": displayrandom_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\"impression_id_test\": impression_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\"product_id_test\": product_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t).reset_index(names=\"obs_index\").assign(model=name,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tfairness_multiplier=l2_fair,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tfairness_fraction=fair_frac,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\titeration=sim),\n",
        "\t\t\t]\n",
        "\t\t)\n",
        "\n",
        "\tresults_df.loc[rows_index] = {\n",
        "\t\t\"NLLH\": nn.CrossEntropyLoss()(y_pred, y_test).item(),\n",
        "\t\t\"DP\": demographic_parity(p, protected_attribute_test, is_senior_test).item(),\n",
        "\t\t\"UTILITY\": utility(\n",
        "\t\t\tp, y_test, protected_attribute_test, impression_test, displayrandom_test\n",
        "\t\t\t).item(),\n",
        "\t\t\"UTILITY_PRODUCT\": utility_product(\n",
        "\t\t\tp, y_test, protected_attribute_test, impression_test, displayrandom_test, product_test, unbiased_ratio=False\n",
        "\t\t\t).item(),\n",
        "\t\t\"UTILITY_PRODUCT_FAIR\": utility_product(\n",
        "\t\t\tp, y_test, protected_attribute_test, impression_test, displayrandom_test, product_test, unbiased_ratio=True\n",
        "\t\t\t).item(),\n",
        "\t\t\"AU-ROC\": roc_auc_score(y_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\ty_score=p[:,1].detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\taverage='macro'),\n",
        "\t\t\"AVG-P-SCORE\": average_precision_score(y_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t   y_score=p[:,1].detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t   average='macro'),\n",
        "\t\t}\n",
        "\tprint(\n",
        "\t\t\"(%20s) NLLH: %.5f DP: %.5f UTILITY: %.5f UTILITY_P: %.5f UTILITY_P_FAIR: %.5f AU-ROC: %.5f AVG-P-SCORE: %.5f\"\n",
        "\t\t% (\n",
        "\t\t\tmodel,\n",
        "\t\t\tresults_df.loc[rows_index,'NLLH'],\n",
        "\t\t\tresults_df.loc[rows_index,'DP'],\n",
        "\t\t\tresults_df.loc[rows_index,'UTILITY'],\n",
        "\t\t\tresults_df.loc[rows_index,'UTILITY_PRODUCT'],\n",
        "\t\t\tresults_df.loc[rows_index,'UTILITY_PRODUCT_FAIR'],\n",
        "\t\t\tresults_df.loc[rows_index,'AU-ROC'],\n",
        "\t\t\tresults_df.loc[rows_index,'AVG-P-SCORE']\n",
        "\t\t\t)\n",
        "\t\t)\n",
        "\treturn res_pred_df\n",
        "\n",
        "\n",
        "def prediction_stats(model, X_test: Tensor, protected_attribute_test: Tensor):\n",
        "\t\"\"\"\n",
        "\tPrints the prediction statistics with respect to protected attribute.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel: Model used for prediction\n",
        "\t\tX_test (Tensor): Test features.\n",
        "\t\tprotected_attribute_test (_type_): Test protected attribute.\n",
        "\t\"\"\"\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\ty_pred = model(X_test)\n",
        "\tres_fit_df = pd.DataFrame({'attribute': protected_attribute_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t   0: torch.softmax(y_pred,dim=1).detach().cpu().numpy()[:,0],\n",
        "\t\t\t\t\t\t\t   1: torch.softmax(y_pred,dim=1).detach().cpu().numpy()[:,1]})\n",
        "\tprint(res_fit_df.set_index('attribute').groupby('attribute').mean())\n",
        "\tprint(torch.softmax(y_pred,dim=1).detach().cpu().numpy()[0,:])\n",
        "\tprint('\\n')"
      ],
      "metadata": {
        "id": "JN01bomayqbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "--------------------------------------------------------------------------------\n",
        "# Run Experiments\n",
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "y1DwJhhsyt39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(args):\n",
        "\t# Copy the main execution logic of your script int(f\"Running experiment with settings: {args}\")\n",
        "\tN_TRAIN_EXAMPLES = 10**5\n",
        "\tDATA_SIZE = None  # None for all data\n",
        "\tsim = 0\n",
        "\tN_EPOCHS = 50\n",
        "\n",
        "\t# Seed for reproducibility\n",
        "\ttorch.manual_seed(43523)\n",
        "\tnp.random.seed(43523)\n",
        "\n",
        "\t# Check for MPS (Metal Performance Shaders) on macOS\n",
        "\tif torch.backends.mps.is_available():\n",
        "\t\tdevice = torch.device(\"mps\")  # Use Metal for PyTorch\n",
        "\t\txgb_device = \"cpu\"  # XGBoost does not support MPS yet, so use CPU\n",
        "\t\txgb_tree_method = \"hist\"  # Use histogram-based tree method for efficiency\n",
        "\telif torch.cuda.is_available():\n",
        "\t\tdevice = torch.device(\"cuda\")  # Use CUDA if available\n",
        "\t\txgb_device = \"cuda\"\n",
        "\t\txgb_tree_method = \"hist\"\n",
        "\telse:\n",
        "\t\tdevice = torch.device(\"cpu\")  # Fallback to CPU\n",
        "\t\txgb_device = \"cpu\"\n",
        "\t\txgb_tree_method = \"hist\"\n",
        "\n",
        "\tprint(f\"Using device: {device}\")\n",
        "\n",
        "\tif not os.path.exists((\"output\")):\n",
        "\t\tos.makedirs((\"output\"))\n",
        "\n",
        "\tif not os.path.exists((\"output/model_hyperparameters\")):\n",
        "\t\tos.makedirs((\"output/model_hyperparameters\"))\n",
        "\n",
        "\n",
        "\tbatch_size = args.batch\n",
        "\tl2_fair_multiplier = args.lambda_fair\n",
        "\tfair_fraction = args.data_frac\n",
        "\tn_trials = args.ntrial\n",
        "\n",
        "\t# Print setup\n",
        "\tprint(args)\n",
        "\tprint('Device: ' + str(device))\n",
        "\n",
        "\t# Name for saving results\n",
        "\targs.name = (\n",
        "\t\t\t\"_\"\n",
        "\t\t\t+ args.name\n",
        "\t\t\t+ \"_lambda\"\n",
        "\t\t\t+ str(args.lambda_fair)\n",
        "\t\t\t+ \"_frac\"\n",
        "\t\t\t+ str(args.data_frac)\n",
        "\t)\n",
        "\n",
        "\n",
        "\t# DataFrame for saving results\n",
        "\tres_pred_df = pd.DataFrame(\n",
        "\t\tcolumns=[\n",
        "\t\t\t\"model\",\n",
        "\t\t\t\"fairness_multiplier\",\n",
        "\t\t\t\"fairness_fraction\",\n",
        "\t\t\t\"obs_index\",\n",
        "\t\t\t\"prob_test\",\n",
        "\t\t\t\"y_test\",\n",
        "\t\t\t\"a_test\",\n",
        "\t\t\t\"s_test\",\n",
        "\t\t\t\"displayrandom_test\",\n",
        "\t\t\t\"impression_id_test\",\n",
        "\t\t\t\"product_id_test\",\n",
        "\t\t\t]\n",
        "\t\t)\n",
        "\n",
        "\t# Data loading and splitting\n",
        "\t(\n",
        "\t\tX,\n",
        "\t\ty,\n",
        "\t\tprotected_attribute,\n",
        "\t\tis_senior,\n",
        "\t\tdisplayrandom,\n",
        "\t\trank,\n",
        "\t\tcategorical_features_cardinalities,\n",
        "\t\t) = load_data(DATA_SIZE, args.data)\n",
        "\n",
        "\tX, y, protected_attribute, is_senior, displayrandom, rank = (\n",
        "\t\tTensor(X.astype(np.float64)).to(device),\n",
        "\t\tTensor(y).long().to(device),\n",
        "\t\tTensor(protected_attribute).long().to(device),\n",
        "\t\tTensor(is_senior).long().to(device),\n",
        "\t\tTensor(displayrandom).long().to(device),\n",
        "\t\tTensor(rank).long().to(device),\n",
        "\t\t)\n",
        "\n",
        "\tif args.unfair == 1:\n",
        "\t\tX = torch.hstack([X, protected_attribute.unsqueeze(dim=1)])\n",
        "\t\targs.name += \"_unfair\"\n",
        "\n",
        "\t(\n",
        "\t\tX_train,\n",
        "\t\tX_test,\n",
        "\t\ty_train,\n",
        "\t\ty_test,\n",
        "\t\tprotected_attribute_train,\n",
        "\t\tprotected_attribute_test,\n",
        "\t\tis_senior_train,\n",
        "\t\tis_senior_test,\n",
        "\t\tdisplayrandom_train,\n",
        "\t\tdisplayrandom_test,\n",
        "\t\trank_train,\n",
        "\t\trank_test,\n",
        "\t\t) = train_test_split(X, y, protected_attribute, is_senior, displayrandom, rank)\n",
        "\n",
        "\tX_extended_train = torch.hstack(\n",
        "\t\t[\n",
        "\t\t\tdisplayrandom_train.unsqueeze(1),\n",
        "\t\t\tis_senior_train.unsqueeze(1),\n",
        "\t\t\tX_train,\n",
        "\t\t\trank_train.unsqueeze(1),\n",
        "\t\t\t]\n",
        "\t\t)\n",
        "\tX_extended_test = torch.hstack(\n",
        "\t\t[displayrandom_test.unsqueeze(1), is_senior_test.unsqueeze(1), X_test, rank_test.unsqueeze(1)]\n",
        "\t\t)\n",
        "\tcategorical_features_cardinalities_extended = {\n",
        "\t\tkey + 2: value for key, value in categorical_features_cardinalities.items()\n",
        "\t\t}\n",
        "\tcategorical_features_cardinalities_extended[0] = 2  # cardinality for displayrandom\n",
        "\tcategorical_features_cardinalities_extended[1] = 2  # cardinality for is_senior\n",
        "\n",
        "\timpression_test = X_test[:, 1]\n",
        "\tproduct_test = X_test[:, 2]\n",
        "\n",
        "\tfair_indicator = (\n",
        "\t\ttorch.bernoulli(fair_fraction * torch.ones(size=y_train.shape)).to(torch.int).to(device)\n",
        "\t)\n",
        "\n",
        "\tdata_train = JobDataset(X_extended_train, y_train, protected_attribute_train, fair_indicator)\n",
        "\n",
        "\n",
        "\t###############\n",
        "\t#### DUMMY ####\n",
        "\t###############\n",
        "\n",
        "\tif args.dummy:\n",
        "\t\tprint(\"\\n RUNNING DUMMY \\n\")\n",
        "\t\tdummy = Learner(DummyClassifier(), device=device)\n",
        "\t\tfor _ in range(N_EPOCHS):\n",
        "\t\t\tdummy.fit(x=X_extended_train, y=y_train, a=protected_attribute_train, batch_size=batch_size)\n",
        "\n",
        "\t\ty_pred = dummy(X_extended_test)\n",
        "\t\tp = torch.softmax(y_pred, dim=1)\n",
        "\t\tres_pred_df = pd.concat(\n",
        "\t\t\t[\n",
        "\t\t\t\tres_pred_df,\n",
        "\t\t\t\tpd.DataFrame(\n",
        "\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\"prob_test\": p[:, 1].detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"y_test\": y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"a_test\": protected_attribute_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"s_test\": is_senior_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"displayrandom_test\": displayrandom_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"impression_id_test\": impression_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"product_id_test\": product_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t.reset_index(names=\"obs_index\")\n",
        "\t\t\t\t.assign(model=\"Dummy\", fairness_multiplier=None, fairness_fraction=None),\n",
        "\t\t\t\t]\n",
        "\t\t\t)\n",
        "\t\tres_pred_df.to_csv((\"output/SINGLE_pred\" + args.name + \".csv\"), mode=\"w+\")\n",
        "\n",
        "\t\tprint(\n",
        "\t\t\t\"DUMMY: NLLH: %.5f DP: %.5f UTILITY: %.5f UTILITY_P: %.5f UTILITY_P_FAIR: %.5f AU-ROC: %.5f AVG-P-SCORE: %.5f \\n\"\n",
        "\t\t\t% (\n",
        "\t\t\t\tnn.CrossEntropyLoss()(y_pred, y_test).item(),\n",
        "\t\t\t\tdemographic_parity(\n",
        "\t\t\t\t\tp,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\tis_senior_test,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility(\n",
        "\t\t\t\t\tp,\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility_product(\n",
        "\t\t\t\t\tp,\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\tproduct_test,\n",
        "\t\t\t\t\tunbiased_ratio=False,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility_product(\n",
        "\t\t\t\t\tp,\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\tproduct_test,\n",
        "\t\t\t\t\tunbiased_ratio=True,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\troc_auc_score(\n",
        "\t\t\t\t\ty_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\ty_score=p[:, 1].detach().cpu().numpy(),\n",
        "\t\t\t\t\taverage=\"macro\",\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\taverage_precision_score(\n",
        "\t\t\t\t\ty_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\ty_score=p[:, 1].detach().cpu().numpy(),\n",
        "\t\t\t\t\taverage=\"macro\",\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t)\n",
        "\t\t\t)\n",
        "\n",
        "\tif args.lr_fair == 0:\n",
        "\t\t\tprint(\"\\n RUNNING LOGISTIC REGRESSION \\n\")\n",
        "\t\t\tdef objective(trial):\n",
        "\t\t\t\t\t(\n",
        "\t\t\t\t\t\t\tX_train_train,\n",
        "\t\t\t\t\t\t\tX_val,\n",
        "\t\t\t\t\t\t\ty_train_train,\n",
        "\t\t\t\t\t\t\ty_val,\n",
        "\t\t\t\t\t\t\tprotected_attribute_train_train,\n",
        "\t\t\t\t\t\t\tprotected_attribute_val,\n",
        "\t\t\t\t\t\t\tis_senior_train_train,\n",
        "\t\t\t\t\t\t\tis_senior_val,\n",
        "\t\t\t\t\t\t\tdisplayrandom_train_train,\n",
        "\t\t\t\t\t\t\tdisplayrandom_val,\n",
        "\t\t\t\t\t\t\trank_train_train,\n",
        "\t\t\t\t\t\t\trank_val,\n",
        "\t\t\t\t\t) = train_test_split(X_train, y_train, protected_attribute_train, is_senior_train, displayrandom_train, rank_train)\n",
        "\t\t\t\t\tX_extended_train_train = torch.hstack(\n",
        "\t\t\t\t\t\t\t[\n",
        "\t\t\t\t\t\t\t\t\tdisplayrandom_train_train.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tis_senior_train_train.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tX_train_train,\n",
        "\t\t\t\t\t\t\t\t\trank_train_train.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t]\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\tX_extended_val = torch.hstack(\n",
        "\t\t\t\t\t\t\t[\n",
        "\t\t\t\t\t\t\t\t\tdisplayrandom_val.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tis_senior_val.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tX_val,\n",
        "\t\t\t\t\t\t\t\t\trank_val.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t]\n",
        "\t\t\t\t\t)\n",
        "\n",
        "\t\t\t\t\temb_size = trial.suggest_int('emb_size',4,5)\n",
        "\t\t\t\t\tlearning_rate = trial.suggest_float('learning_rate',1e-4, 1e-2,log=True)\n",
        "\t\t\t\t\tweight_decay = trial.suggest_float('weight_decay',1e-6, 1e-4,log=True)\n",
        "\t\t\t\t\tscheduler_step_size = trial.suggest_int('scheduler_step_size',20,N_EPOCHS)\n",
        "\t\t\t\t\tscheduler_gamma = trial.suggest_float('scheduler_gamma',1e-2,1,log=True)\n",
        "\n",
        "\t\t\t\t\tlr = Learner(\n",
        "\t\t\t\t\t\t\tLogisticRegression(X_extended_train_train.shape[1], categorical_features_cardinalities_extended, emb_size),\n",
        "\t\t\t\t\t\t\tdevice=device,\n",
        "\t\t\t\t\t\t\tscheduler_step_size=scheduler_step_size,\n",
        "\t\t\t\t\t\t\tscheduler_gamma=scheduler_gamma,\n",
        "\t\t\t\t\t\t\tlr=learning_rate,\n",
        "\t\t\t\t\t\t\tweight_decay=weight_decay\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\tfor _ in range(N_EPOCHS):\n",
        "\t\t\t\t\t\t\tif _ * batch_size > N_TRAIN_EXAMPLES:\n",
        "\t\t\t\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\t\t\tlr.fit(\n",
        "\t\t\t\t\t\t\t\t\tx = X_extended_train_train,\n",
        "\t\t\t\t\t\t\t\t\ty = y_train_train,\n",
        "\t\t\t\t\t\t\t\t\ta = protected_attribute_train_train,\n",
        "\t\t\t\t\t\t\t\t\tbatch_size = batch_size,\n",
        "\t\t\t\t\t\t\t)\n",
        "\t\t\t\t\t\t\tlr.scheduler_step()\n",
        "\n",
        "\t\t\t\t\t\t\ty_pred = lr(X_extended_val)\n",
        "\t\t\t\t\t\t\tintermediate_value = nn.CrossEntropyLoss()(y_pred, y_val).item()\n",
        "\t\t\t\t\t\t\ttrial.report(intermediate_value, _)\n",
        "\t\t\t\t\t\t\tif trial.should_prune():\n",
        "\t\t\t\t\t\t\t\t\traise optuna.TrialPruned()\n",
        "\n",
        "\t\t\t\t\ty_pred = lr(X_extended_val)\n",
        "\t\t\t\t\treturn nn.CrossEntropyLoss()(y_pred, y_val).item()\n",
        "\n",
        "\t\t\tstudy = optuna.create_study(direction='minimize')\n",
        "\t\t\tstudy.optimize(objective, n_trials=n_trials)\n",
        "\t\t\toptimal = study.best_trial\n",
        "\t\t\tpd.DataFrame(optimal.params, index=[sim]).to_csv(\n",
        "\t\t\t\t\t(\"output/model_hyperparameters/LR_opt_params\" + args.name + \".csv\"),\n",
        "\t\t\t\t\tmode='a', header=not (\"output/model_hyperparameters/LR_opt_params\" + args.name + \".csv\")\n",
        "\t\t\t)\n",
        "\t\t\tresults_regular_df = pd.DataFrame()\n",
        "\t\t\t# Fit on all training data\n",
        "\t\t\tlr = Learner(\n",
        "\t\t\t\t\tLogisticRegression(\n",
        "\t\t\t\t\t\t\tX_extended_train.shape[1],\n",
        "\t\t\t\t\t\t\tcategorical_features_cardinalities_extended,\n",
        "\t\t\t\t\t\t\toptimal.params[\"emb_size\"],\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tdevice=device,\n",
        "\t\t\t\t\tscheduler_step_size=optimal.params['scheduler_step_size'],\n",
        "\t\t\t\t\tscheduler_gamma=optimal.params['scheduler_gamma'],\n",
        "\t\t\t\t\tlr=optimal.params[\"learning_rate\"],\n",
        "\t\t\t\t\tweight_decay=optimal.params[\"weight_decay\"],\n",
        "\t\t\t)\n",
        "\t\t\tfor _ in range(N_EPOCHS):\n",
        "\t\t\t\t\tlr.fit(\n",
        "\t\t\t\t\t\t\tx=X_extended_train,\n",
        "\t\t\t\t\t\t\ty=y_train,\n",
        "\t\t\t\t\t\t\ta=protected_attribute_train,\n",
        "\t\t\t\t\t\t\tbatch_size=batch_size,\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\tlr.scheduler_step()\n",
        "\n",
        "\t\t\tres_pred_df = pd.concat(\n",
        "\t\t\t[\n",
        "\t\t\t\tres_pred_df,\n",
        "\t\t\t\tpd.DataFrame(\n",
        "\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\"prob_test\": p[:, 1].detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"y_test\": y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"a_test\": protected_attribute_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"s_test\": is_senior_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"displayrandom_test\": displayrandom_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"impression_id_test\": impression_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"product_id_test\": product_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t.reset_index(names=\"obs_index\")\n",
        "\t\t\t\t.assign(\n",
        "\t\t\t\t\tmodel=\"LR\", fairness_multiplier=l2_fair_multiplier, fairness_fraction=fair_fraction\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t]\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\t# Intermediate save\n",
        "\t\t\tres_pred_df.to_csv(('output/SINGLE_PRED_LR' + args.name + '.csv'), mode='w+')\n",
        "\t\t\tprint(\n",
        "\t\t\t\"\\n LR: NLLH: %.5f DP: %.5f UTILITY: %.5f UTILITY_P: %.5f UTILITY_P_FAIR: %.5f AU-ROC: %.5f AVG-P-SCORE: %.5f \\n\"\n",
        "\t\t\t% (\n",
        "\t\t\t\tlog_loss(y_test.detach().cpu().numpy(), prob_test),\n",
        "\t\t\t\tdemographic_parity(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\tis_senior_test,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility_product(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\tproduct_test,\n",
        "\t\t\t\t\tunbiased_ratio=False,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility_product(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\tproduct_test,\n",
        "\t\t\t\t\tunbiased_ratio=True,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\troc_auc_score(\n",
        "\t\t\t\t\ty_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\ty_score=prob_test[:, 1],\n",
        "\t\t\t\t\taverage=\"macro\",\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\taverage_precision_score(\n",
        "\t\t\t\t\ty_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\ty_score=prob_test[:, 1],\n",
        "\t\t\t\t\taverage=\"macro\",\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t)\n",
        "\t\t\t)\n",
        "\n",
        "\n",
        "\t\t# FAIR LOGISTIC REGRESSION\n",
        "\tif args.lr_fair:\n",
        "\t\t\tprint(\n",
        "\t\t\t\t\tf\"\\n RUNNING FAIR LOGISTIC REGRESSION with lambda {l2_fair_multiplier} and frac {fair_fraction} \\n\"\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\tdef objective(trial):\n",
        "\t\t\t\t\t(\n",
        "\t\t\t\t\t\t\tX_train_train,\n",
        "\t\t\t\t\t\t\tX_val,\n",
        "\t\t\t\t\t\t\ty_train_train,\n",
        "\t\t\t\t\t\t\ty_val,\n",
        "\t\t\t\t\t\t\tprotected_attribute_train_train,\n",
        "\t\t\t\t\t\t\tprotected_attribute_val,\n",
        "\t\t\t\t\t\t\tis_senior_train_train,\n",
        "\t\t\t\t\t\t\tis_senior_val,\n",
        "\t\t\t\t\t\t\tdisplayrandom_train_train,\n",
        "\t\t\t\t\t\t\tdisplayrandom_val,\n",
        "\t\t\t\t\t\t\trank_train_train,\n",
        "\t\t\t\t\t\t\trank_val,\n",
        "\t\t\t\t\t) = train_test_split(\n",
        "\t\t\t\t\t\t\tX_train,\n",
        "\t\t\t\t\t\t\ty_train,\n",
        "\t\t\t\t\t\t\tprotected_attribute_train,\n",
        "\t\t\t\t\t\t\tis_senior_train,\n",
        "\t\t\t\t\t\t\tdisplayrandom_train,\n",
        "\t\t\t\t\t\t\trank_train,\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\tX_extended_train_train = torch.hstack(\n",
        "\t\t\t\t\t\t\t[\n",
        "\t\t\t\t\t\t\t\t\tdisplayrandom_train_train.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tis_senior_train_train.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tX_train_train,\n",
        "\t\t\t\t\t\t\t\t\trank_train_train.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t]\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\tX_extended_val = torch.hstack(\n",
        "\t\t\t\t\t\t\t[\n",
        "\t\t\t\t\t\t\t\t\tdisplayrandom_val.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tis_senior_val.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t\t\tX_val,\n",
        "\t\t\t\t\t\t\t\t\trank_val.unsqueeze(1),\n",
        "\t\t\t\t\t\t\t]\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\tfair_indicator = (\n",
        "\t\t\t\t\t\t\ttorch.bernoulli(fair_fraction * torch.ones(size=y_train_train.shape))\n",
        "\t\t\t\t\t\t\t.to(torch.int)\n",
        "\t\t\t\t\t\t\t.to(device)\n",
        "\t\t\t\t\t)\n",
        "\n",
        "\t\t\t\t\temb_size = trial.suggest_int(\"emb_size\", 4, 8)\n",
        "\t\t\t\t\tlearning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
        "\t\t\t\t\tweight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
        "\t\t\t\t\tfair_lr = Learner(\n",
        "\t\t\t\t\t\t\tLogisticRegression(\n",
        "\t\t\t\t\t\t\t\t\tX_extended_train_train.shape[1],\n",
        "\t\t\t\t\t\t\t\t\tcategorical_features_cardinalities_extended,\n",
        "\t\t\t\t\t\t\t\t\tembedding_size=emb_size,\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\tdevice=device,\n",
        "\t\t\t\t\t\t\tbasename=\"L2 FAIR\",\n",
        "\t\t\t\t\t\t\tlr=learning_rate,\n",
        "\t\t\t\t\t\t\tweight_decay=weight_decay,\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\tfor _ in range(N_EPOCHS):\n",
        "\t\t\t\t\t\t\tif _ * batch_size > N_TRAIN_EXAMPLES:\n",
        "\t\t\t\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\t\t\tfair_lr.fit(\n",
        "\t\t\t\t\t\t\t\t\tx=X_extended_train_train,\n",
        "\t\t\t\t\t\t\t\t\ty=y_train_train,\n",
        "\t\t\t\t\t\t\t\t\ta=protected_attribute_train_train,\n",
        "\t\t\t\t\t\t\t\t\tpenalty_fun=l2_conditional_independence_penalty,\n",
        "\t\t\t\t\t\t\t\t\tpenalty_multiplier=l2_fair_multiplier,\n",
        "\t\t\t\t\t\t\t\t\tfair_indicator=fair_indicator,\n",
        "\t\t\t\t\t\t\t\t\tbatch_size=batch_size,\n",
        "\t\t\t\t\t\t\t)\n",
        "\t\t\t\t\ty_pred = fair_lr(X_extended_val)\n",
        "\n",
        "\t\t\t\t\treturn nn.CrossEntropyLoss()(y_pred, y_val).item()\n",
        "\n",
        "\t\t\tstudy = optuna.create_study(direction=\"minimize\")\n",
        "\t\t\tstudy.optimize(objective, n_trials=n_trials)\n",
        "\t\t\toptimal = study.best_trial\n",
        "\t\t\tpd.DataFrame(\n",
        "\t\t\t\t\toptimal.params,\n",
        "\t\t\t\t\tindex=pd.MultiIndex.from_product([[fair_fraction], [sim]], names=[\"fair_fraction\", \"sim\"]),\n",
        "\t\t\t).to_csv(\n",
        "\t\t\t\t\t(\n",
        "\t\t\t\t\t\t\t\"output/model_hyperparameters/SINGLE_FAIR_LR_W_opt_params\" + args.name + \".csv\"\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tmode=\"a\",\n",
        "\t\t\t\t\theader=not (\n",
        "\t\t\t\t\t\t\t\"output/model_hyperparameters/SINGLE_FAIR_LR_W_opt_params\" + args.name + \".csv\"\n",
        "\t\t\t\t\t),\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\tfair_lr = Learner(\n",
        "\t\t\t\t\tLogisticRegression(\n",
        "\t\t\t\t\t\t\tX_extended_train.shape[1],\n",
        "\t\t\t\t\t\t\tcategorical_features_cardinalities_extended,\n",
        "\t\t\t\t\t\t\tembedding_size=optimal.params[\"emb_size\"],\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tdevice=device,\n",
        "\t\t\t\t\tbasename=\"L2 FAIR\",\n",
        "\t\t\t\t\tlr=optimal.params[\"learning_rate\"],\n",
        "\t\t\t\t\tweight_decay=optimal.params[\"weight_decay\"],\n",
        "\t\t\t)\n",
        "\t\t\tfor _ in range(N_EPOCHS):\n",
        "\t\t\t\t\tprint(\"EPOCHS: \" + str(_ + 1) + \"/\" + str(N_EPOCHS) + \"\\n\")\n",
        "\t\t\t\t\tfair_lr.fit(\n",
        "\t\t\t\t\t\t\tx=X_extended_train,\n",
        "\t\t\t\t\t\t\ty=y_train,\n",
        "\t\t\t\t\t\t\ta=protected_attribute_train,\n",
        "\t\t\t\t\t\t\tpenalty_fun=l2_conditional_independence_penalty,\n",
        "\t\t\t\t\t\t\tpenalty_multiplier=l2_fair_multiplier,\n",
        "\t\t\t\t\t\t\tbatch_size=batch_size,\n",
        "\t\t\t\t\t\t\tfair_indicator=fair_indicator,\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\t\t\ttmp = fair_lr(X_extended_train)\n",
        "\t\t\t\t\t\t\ttmp_test = fair_lr(X_extended_test)\n",
        "\t\t\t\t\tprint(f\"training NLLH : {nn.CrossEntropyLoss()(tmp,y_train).item()}\")\n",
        "\t\t\t\t\tprint(f\"test NLLH : {nn.CrossEntropyLoss()(tmp_test,y_test).item()}\")\n",
        "\n",
        "\t\t\ty_pred = fair_lr(X_extended_test)\n",
        "\t\t\tp = torch.softmax(y_pred, dim=1)\n",
        "\t\t\tres_pred_df = pd.concat(\n",
        "\t\t\t\t\t[\n",
        "\t\t\t\t\t\t\tres_pred_df,\n",
        "\t\t\t\t\t\t\tpd.DataFrame(\n",
        "\t\t\t\t\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"prob_test\": p[:, 1].detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"y_test\": y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"a_test\": protected_attribute_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"s_test\": is_senior_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"displayrandom_test\": displayrandom_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"impression_id_test\": impression_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"product_id_test\": product_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t)\n",
        "\t\t\t\t\t\t\t.reset_index(names=\"obs_index\")\n",
        "\t\t\t\t\t\t\t.assign(\n",
        "\t\t\t\t\t\t\t\t\tmodel=\"LR\", fairness_multiplier=l2_fair_multiplier, fairness_fraction=fair_fraction\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t]\n",
        "\t\t\t)\n",
        "\t\t\tres_pred_df.to_csv((\"output/SINGLE_pred\" + args.name + \".csv\"), mode=\"w+\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t#################\n",
        "\t#### XGBOOST ####\n",
        "\t#################\n",
        "\n",
        "\tif args.xgb:\n",
        "\t\tprint(\"\\n RUNNING XGBOOST \\n\")\n",
        "\t\tcat_cols = list(categorical_features_cardinalities_extended.keys())\n",
        "\n",
        "\t\tenc_auto = TargetEncoder(target_type=\"binary\")\n",
        "\t\tenc_auto.fit(\n",
        "\t\t\tX_extended_train[:, cat_cols].detach().cpu().numpy(), y_train.detach().cpu().numpy()\n",
        "\t\t\t)\n",
        "\n",
        "\t\tdef objective(trial):\n",
        "\t\t\t(\n",
        "\t\t\t\tX_train_train,\n",
        "\t\t\t\tX_val,\n",
        "\t\t\t\ty_train_train,\n",
        "\t\t\t\ty_val,\n",
        "\t\t\t\tprotected_attribute_train_train,\n",
        "\t\t\t\tprotected_attribute_val,\n",
        "\t\t\t\tis_senior_train_train,\n",
        "\t\t\t\tis_senior_val,\n",
        "\t\t\t\tdisplayrandom_train_train,\n",
        "\t\t\t\tdisplayrandom_val,\n",
        "\t\t\t\trank_train_train,\n",
        "\t\t\t\trank_val,\n",
        "\t\t\t\t) = train_test_split(\n",
        "\t\t\t\tX_train,\n",
        "\t\t\t\ty_train,\n",
        "\t\t\t\tprotected_attribute_train,\n",
        "\t\t\t\tis_senior_train,\n",
        "\t\t\t\tdisplayrandom_train,\n",
        "\t\t\t\trank_train,\n",
        "\t\t\t\t)\n",
        "\t\t\tX_extended_train_train = torch.hstack(\n",
        "\t\t\t\t[displayrandom_train_train.unsqueeze(1), X_train_train, rank_train_train.unsqueeze(1)]\n",
        "\t\t\t\t)\n",
        "\t\t\tX_extended_val = torch.hstack(\n",
        "\t\t\t\t[displayrandom_val.unsqueeze(1), X_val, rank_val.unsqueeze(1)]\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\t\tmax_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "\t\t\tmin_child_weight = trial.suggest_float(\"min_child_weight\", 0.0001, 100, log=True)\n",
        "\t\t\tsubsample = trial.suggest_float(\"subsample\", 0.5, 1)\n",
        "\t\t\tlearning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1, log=True)\n",
        "\t\t\tcolsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1)\n",
        "\t\t\treg_lambda = trial.suggest_float(\"reg_lambda\", 0.1, 10, log=True)\n",
        "\t\t\tgamma = trial.suggest_float(\"gamma\", 0.001, 100, log=True)\n",
        "\n",
        "\t\t\tscale_pos_weight = (\n",
        "\t\t\t\t(torch.sum(y_train_train < 1) / torch.sum(y_train_train > 0)).detach().cpu().numpy()\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\tX_xgb_train = np.hstack(\n",
        "\t\t\t\t[\n",
        "\t\t\t\t\tenc_auto.transform(X_extended_train_train[:, cat_cols].detach().cpu().numpy()),\n",
        "\t\t\t\t\tX_extended_train_train[:, (max(cat_cols) + 1) :].detach().cpu().numpy(),\n",
        "\t\t\t\t\t]\n",
        "\t\t\t\t)\n",
        "\t\t\tX_xgb_val = np.hstack(\n",
        "\t\t\t\t[\n",
        "\t\t\t\t\tenc_auto.transform(X_extended_val[:, cat_cols].detach().cpu().numpy()),\n",
        "\t\t\t\t\tX_extended_val[:, (max(cat_cols) + 1) :].detach().cpu().numpy(),\n",
        "\t\t\t\t\t]\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\t\tmodel_xgb = xgb.XGBClassifier(\n",
        "\t\t\t\tscale_pos_weight=scale_pos_weight,\n",
        "\t\t\t\ttree_method=xgb_tree_method,\n",
        "\t\t\t\tdevice=xgb_device,\n",
        "\t\t\t\tmax_depth=max_depth,\n",
        "\t\t\t\tmin_child_weight=min_child_weight,\n",
        "\t\t\t\tsubsample=subsample,\n",
        "\t\t\t\tlearning_rate=learning_rate,\n",
        "\t\t\t\tgamma=gamma,\n",
        "\t\t\t\tcolsample_bytree=colsample_bytree,\n",
        "\t\t\t\treg_lambda=reg_lambda,\n",
        "\t\t\t\t)\n",
        "\t\t\tmodel_xgb.fit(X_xgb_train, y_train_train.detach().cpu().numpy())\n",
        "\t\t\tprob_val = model_xgb.predict_proba(X_xgb_val)\n",
        "\t\t\treturn log_loss(y_val.detach().cpu().numpy(), prob_val)\n",
        "\n",
        "\t\tstudy = optuna.create_study(direction=\"minimize\")\n",
        "\t\tstudy.optimize(objective, n_trials=n_trials)\n",
        "\t\toptimal = study.best_trial\n",
        "\t\tpd.DataFrame(optimal.params, index=[sim]).to_csv(\n",
        "\t\t\t(\"output/model_hyperparameters/XGBoost_opt_params\" + args.name + \".csv\"),\n",
        "\t\t\tmode=\"a\",\n",
        "\t\t\theader=not (\n",
        "\t\t\t\t\"output/model_hyperparameters/XGBoost_opt_params\" + args.name + \".csv\"\n",
        "\t\t\t\t),\n",
        "\t\t\t)\n",
        "\t\tscale_pos_weight = (torch.sum(y_train < 1) / torch.sum(y_train > 0)).detach().cpu().numpy()\n",
        "\t\tX_xgb_train = np.hstack(\n",
        "\t\t\t[\n",
        "\t\t\t\tenc_auto.transform(X_extended_train[:, cat_cols].detach().cpu().numpy()),\n",
        "\t\t\t\tX_extended_train[:, (max(cat_cols) + 1) :].detach().cpu().numpy(),\n",
        "\t\t\t\t]\n",
        "\t\t\t)\n",
        "\t\tX_xgb_test = np.hstack(\n",
        "\t\t\t[\n",
        "\t\t\t\tenc_auto.transform(X_extended_test[:, cat_cols].detach().cpu().numpy()),\n",
        "\t\t\t\tX_extended_test[:, (max(cat_cols) + 1) :].detach().cpu().numpy(),\n",
        "\t\t\t\t]\n",
        "\t\t\t)\n",
        "\t\tmodel_xgb = xgb.XGBClassifier(\n",
        "\t\t\tscale_pos_weight=scale_pos_weight,\n",
        "\t\t\ttree_method=xgb_tree_method,\n",
        "\t\t\tdevice=xgb_device,\n",
        "\t\t\tmax_depth=optimal.params[\"max_depth\"],\n",
        "\t\t\tmin_child_weight=optimal.params[\"min_child_weight\"],\n",
        "\t\t\tgamma=optimal.params[\"gamma\"],\n",
        "\t\t\tsubsample=optimal.params[\"subsample\"],\n",
        "\t\t\tlearning_rate=optimal.params[\"learning_rate\"],\n",
        "\t\t\tcolsample_bytree=optimal.params[\"colsample_bytree\"],\n",
        "\t\t\treg_lambda=optimal.params[\"reg_lambda\"],\n",
        "\t\t\t)\n",
        "\t\tmodel_xgb.fit(X_extended_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
        "\n",
        "\t\tif args.unfair == 0:\n",
        "\t\t\tmodel_xgb.save_model((\"output/XGB_single_fit.model\"))\n",
        "\t\telse:\n",
        "\t\t\tmodel_xgb.save_model((\"output/XGB_single_fit_unfair.model\"))\n",
        "\n",
        "\t\tprob_test = model_xgb.predict_proba(X_extended_test.detach().cpu().numpy())\n",
        "\n",
        "\t\tres_pred_df = pd.concat(\n",
        "\t\t\t[\n",
        "\t\t\t\tres_pred_df,\n",
        "\t\t\t\tpd.DataFrame(\n",
        "\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\"prob_test\": prob_test[:, 1],\n",
        "\t\t\t\t\t\t\"y_test\": y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"a_test\": protected_attribute_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"s_test\": is_senior_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"displayrandom_test\": displayrandom_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"impression_id_test\": impression_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t\"product_id_test\": product_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\t.reset_index(names=\"obs_index\")\n",
        "\t\t\t\t.assign(model=\"XGB\", fairness_multiplier=None, fairness_fraction=None),\n",
        "\t\t\t\t]\n",
        "\t\t\t)\n",
        "\t\tres_pred_df.to_csv((\"output/SINGLE_pred\" + args.name + \".csv\"), mode=\"w+\")\n",
        "\n",
        "\t\tprint(\n",
        "\t\t\t\"\\nXGBoost: NLLH: %.5f DP: %.5f UTILITY: %.5f UTILITY_P: %.5f UTILITY_P_FAIR: %.5f AU-ROC: %.5f AVG-P-SCORE: %.5f \\n\"\n",
        "\t\t\t% (\n",
        "\t\t\t\tlog_loss(y_test.detach().cpu().numpy(), prob_test),\n",
        "\t\t\t\tdemographic_parity(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\tis_senior_test,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility_product(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\tproduct_test,\n",
        "\t\t\t\t\tunbiased_ratio=False,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\tutility_product(\n",
        "\t\t\t\t\tTensor(prob_test.astype(np.float64)).to(device),\n",
        "\t\t\t\t\ty_test,\n",
        "\t\t\t\t\tprotected_attribute_test,\n",
        "\t\t\t\t\timpression_test,\n",
        "\t\t\t\t\tdisplayrandom_test,\n",
        "\t\t\t\t\tproduct_test,\n",
        "\t\t\t\t\tunbiased_ratio=True,\n",
        "\t\t\t\t\t).item(),\n",
        "\t\t\t\troc_auc_score(\n",
        "\t\t\t\t\ty_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\ty_score=prob_test[:, 1],\n",
        "\t\t\t\t\taverage=\"macro\",\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\taverage_precision_score(\n",
        "\t\t\t\t\ty_true=y_test.detach().cpu().numpy(),\n",
        "\t\t\t\t\ty_score=prob_test[:, 1],\n",
        "\t\t\t\t\taverage=\"macro\",\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t)\n",
        "\t\t\t)\n",
        "\n",
        "\n",
        "\t# Final save\n",
        "\tres_pred_df.to_csv((\"output/SINGLE_pred\" + args.name + \".csv\"), mode=\"w+\")\n",
        "\t\t# Your existing script logic goes\n",
        "\t\t# Load data, process, train models, save results, etc.\n",
        "\n",
        "\tprint(f\"Experiment {args.name} completed.\\n\")\n"
      ],
      "metadata": {
        "id": "_iM-Wrbhku1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# Define a list of different experiment settings\n",
        "experiment_configs = [\n",
        "\t# Namespace(batch=2048, lambda_fair=0.1, data_frac=1.0, ntrial=1, dummy=1, lr_fair=0, xgb=0, name=\"DUMMY\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=0.1, data_frac=1.0, ntrial=1, dummy=0, lr_fair=0, xgb=0, name=\"LR_UNAWARE\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=0.1, data_frac=1.0, ntrial=1, dummy=0, lr_fair=0, xgb=0, name=\"LR_UNFAIR\", unfair=1, data=\"fairjob.csv.gz\"),\n",
        "\tNamespace(batch=2048, lambda_fair=3.0, data_frac=1.0, ntrial=1, dummy=0, lr_fair=1, xgb=0, name=\"LR_FAIR\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=0.1, data_frac=1.0, ntrial=1, dummy=0, lr_fair=0, xgb=1, name=\"XGB_UNAWARE\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=0.1, data_frac=1.0, ntrial=1, dummy=0, lr_fair=0, xgb=1, name=\"XGB_UNFAIR\", unfair=1, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=0.1, data_frac=1.0, ntrial=1, dummy=0, lr_fair=1, xgb=0, name=\"LR_FAIR_0.1\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=0.316, data_frac=1.0, ntrial=1, dummy=0, lr_fair=1, xgb=0, name=\"LR_FAIR_0.316\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=0.5, data_frac=1.0, ntrial=1, dummy=0, lr_fair=1, xgb=0, name=\"LR_FAIR_0.5\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=1.0, data_frac=1.0, ntrial=1, dummy=0, lr_fair=1, xgb=0, name=\"LR_FAIR_1.0\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t# Namespace(batch=2048, lambda_fair=10.0, data_frac=1.0, ntrial=1, dummy=0, lr_fair=1, xgb=0, name=\"LR_FAIR_10.0\", unfair=0, data=\"fairjob.csv.gz\"),\n",
        "\t]\n",
        "\n",
        "# Loop through and execute each experiment\n",
        "for args in experiment_configs:\n",
        "\tprint(f\"\\n=== Running Experiment: {args.name} ===\\n\")\n",
        "\n",
        "\t# Call your main script logic rectly use your existing code with args)\n",
        "\t# This assumes that your script is inside a function named `run_experiment(args)`\n",
        "\trun_experiment(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyJETFcElFJT",
        "outputId": "0632d597-55b0-467e-c8b4-0ea11ca87d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Experiment: LR_FAIR ===\n",
            "\n",
            "Using device: cuda\n",
            "Namespace(batch=2048, lambda_fair=3.0, data_frac=1.0, ntrial=1, dummy=0, lr_fair=1, xgb=0, name='LR_FAIR', unfair=0, data='fairjob.csv.gz')\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-10 07:21:54,459] A new study created in memory with name: no-name-a4ff5d4a-37aa-4a7e-9f70-1d46961956e7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " RUNNING FAIR LOGISTIC REGRESSION with lambda 3.0 and frac 1.0 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-10 07:33:35,423] Trial 0 finished with value: 0.4578497111797333 and parameters: {'emb_size': 6, 'learning_rate': 0.00023703739287657093, 'weight_decay': 7.4938440556214405e-06}. Best is trial 0 with value: 0.4578497111797333.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCHS: 1/50\n",
            "\n",
            "training NLLH : 0.6429318785667419\n",
            "test NLLH : 0.6425785422325134\n",
            "EPOCHS: 2/50\n",
            "\n",
            "training NLLH : 0.6416888236999512\n",
            "test NLLH : 0.6413614749908447\n",
            "EPOCHS: 3/50\n",
            "\n",
            "training NLLH : 0.6246672868728638\n",
            "test NLLH : 0.6245506405830383\n",
            "EPOCHS: 4/50\n",
            "\n",
            "training NLLH : 0.6348312497138977\n",
            "test NLLH : 0.6348016262054443\n",
            "EPOCHS: 5/50\n",
            "\n",
            "training NLLH : 0.6234111189842224\n",
            "test NLLH : 0.6233663558959961\n",
            "EPOCHS: 6/50\n",
            "\n",
            "training NLLH : 0.6256871223449707\n",
            "test NLLH : 0.6256309747695923\n",
            "EPOCHS: 7/50\n",
            "\n",
            "training NLLH : 0.6140580773353577\n",
            "test NLLH : 0.6140996813774109\n",
            "EPOCHS: 8/50\n",
            "\n",
            "training NLLH : 0.6262333989143372\n",
            "test NLLH : 0.6263978481292725\n",
            "EPOCHS: 9/50\n",
            "\n",
            "training NLLH : 0.6041541695594788\n",
            "test NLLH : 0.6044495701789856\n",
            "EPOCHS: 10/50\n",
            "\n",
            "training NLLH : 0.6075776815414429\n",
            "test NLLH : 0.60796719789505\n",
            "EPOCHS: 11/50\n",
            "\n",
            "training NLLH : 0.5861515402793884\n",
            "test NLLH : 0.5866791605949402\n",
            "EPOCHS: 12/50\n",
            "\n",
            "training NLLH : 0.6099385023117065\n",
            "test NLLH : 0.6107441186904907\n",
            "EPOCHS: 13/50\n",
            "\n",
            "training NLLH : 0.5764665007591248\n",
            "test NLLH : 0.5773859024047852\n",
            "EPOCHS: 14/50\n",
            "\n",
            "training NLLH : 0.5758830904960632\n",
            "test NLLH : 0.5771741271018982\n",
            "EPOCHS: 15/50\n",
            "\n",
            "training NLLH : 0.5528307557106018\n",
            "test NLLH : 0.5547865629196167\n",
            "EPOCHS: 16/50\n",
            "\n",
            "training NLLH : 0.535400927066803\n",
            "test NLLH : 0.5379232168197632\n",
            "EPOCHS: 17/50\n",
            "\n",
            "training NLLH : 0.535352349281311\n",
            "test NLLH : 0.5384304523468018\n",
            "EPOCHS: 18/50\n",
            "\n",
            "training NLLH : 0.5287930369377136\n",
            "test NLLH : 0.532680094242096\n",
            "EPOCHS: 19/50\n",
            "\n",
            "training NLLH : 0.5176398158073425\n",
            "test NLLH : 0.5222061276435852\n",
            "EPOCHS: 20/50\n",
            "\n",
            "training NLLH : 0.4784977436065674\n",
            "test NLLH : 0.4837098717689514\n",
            "EPOCHS: 21/50\n",
            "\n",
            "training NLLH : 0.4946935474872589\n",
            "test NLLH : 0.5005736947059631\n",
            "EPOCHS: 22/50\n",
            "\n",
            "training NLLH : 0.4703868627548218\n",
            "test NLLH : 0.47665244340896606\n",
            "EPOCHS: 23/50\n",
            "\n",
            "training NLLH : 0.455167293548584\n",
            "test NLLH : 0.4616866111755371\n",
            "EPOCHS: 24/50\n",
            "\n",
            "training NLLH : 0.4570102393627167\n",
            "test NLLH : 0.46411439776420593\n",
            "EPOCHS: 25/50\n",
            "\n",
            "training NLLH : 0.450278103351593\n",
            "test NLLH : 0.45773571729660034\n",
            "EPOCHS: 26/50\n",
            "\n",
            "training NLLH : 0.4724794328212738\n",
            "test NLLH : 0.4805879294872284\n",
            "EPOCHS: 27/50\n",
            "\n",
            "training NLLH : 0.4497816264629364\n",
            "test NLLH : 0.45807069540023804\n",
            "EPOCHS: 28/50\n",
            "\n",
            "training NLLH : 0.4094979465007782\n",
            "test NLLH : 0.41741031408309937\n",
            "EPOCHS: 29/50\n",
            "\n",
            "training NLLH : 0.42102858424186707\n",
            "test NLLH : 0.42919376492500305\n",
            "EPOCHS: 30/50\n",
            "\n",
            "training NLLH : 0.4228599965572357\n",
            "test NLLH : 0.4313240647315979\n",
            "EPOCHS: 31/50\n",
            "\n",
            "training NLLH : 0.4240361154079437\n",
            "test NLLH : 0.4323415458202362\n",
            "EPOCHS: 32/50\n",
            "\n",
            "training NLLH : 0.483602911233902\n",
            "test NLLH : 0.49241310358047485\n",
            "EPOCHS: 33/50\n",
            "\n",
            "training NLLH : 0.43417373299598694\n",
            "test NLLH : 0.44270390272140503\n",
            "EPOCHS: 34/50\n",
            "\n",
            "training NLLH : 0.45074278116226196\n",
            "test NLLH : 0.4596007168292999\n",
            "EPOCHS: 35/50\n",
            "\n",
            "training NLLH : 0.4427703619003296\n",
            "test NLLH : 0.45160573720932007\n",
            "EPOCHS: 36/50\n",
            "\n",
            "training NLLH : 0.46286046504974365\n",
            "test NLLH : 0.47170424461364746\n",
            "EPOCHS: 37/50\n",
            "\n",
            "training NLLH : 0.4262707829475403\n",
            "test NLLH : 0.43498265743255615\n",
            "EPOCHS: 38/50\n",
            "\n",
            "training NLLH : 0.40358972549438477\n",
            "test NLLH : 0.41198617219924927\n",
            "EPOCHS: 39/50\n",
            "\n",
            "training NLLH : 0.41118869185447693\n",
            "test NLLH : 0.41958877444267273\n",
            "EPOCHS: 40/50\n",
            "\n",
            "training NLLH : 0.4096906781196594\n",
            "test NLLH : 0.4181784391403198\n",
            "EPOCHS: 41/50\n",
            "\n",
            "training NLLH : 0.42866000533103943\n",
            "test NLLH : 0.4372923672199249\n",
            "EPOCHS: 42/50\n",
            "\n",
            "training NLLH : 0.4025871753692627\n",
            "test NLLH : 0.41118699312210083\n",
            "EPOCHS: 43/50\n",
            "\n",
            "training NLLH : 0.418986976146698\n",
            "test NLLH : 0.42767906188964844\n",
            "EPOCHS: 44/50\n",
            "\n",
            "training NLLH : 0.4435541033744812\n",
            "test NLLH : 0.45240530371665955\n",
            "EPOCHS: 45/50\n",
            "\n",
            "training NLLH : 0.4152468144893646\n",
            "test NLLH : 0.42392924427986145\n",
            "EPOCHS: 46/50\n",
            "\n",
            "training NLLH : 0.4145812392234802\n",
            "test NLLH : 0.42314258217811584\n",
            "EPOCHS: 47/50\n",
            "\n",
            "training NLLH : 0.4507376551628113\n",
            "test NLLH : 0.45955654978752136\n",
            "EPOCHS: 48/50\n",
            "\n",
            "training NLLH : 0.377958208322525\n",
            "test NLLH : 0.3863290548324585\n",
            "EPOCHS: 49/50\n",
            "\n",
            "training NLLH : 0.431499719619751\n",
            "test NLLH : 0.4400809705257416\n",
            "EPOCHS: 50/50\n",
            "\n",
            "training NLLH : 0.4315080940723419\n",
            "test NLLH : 0.4403681755065918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-dac44a79d0ef>:512: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  res_pred_df = pd.concat(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment _LR_FAIR_lambda3.0_frac1.0 completed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame({\n",
        "    \"MODEL\": [\"Dummy\", \"XGBoost UNAWARE\", \"XGBoost UNFAIR\"],\n",
        "    \"NLLH\": [0.69404, 0.43431, 0.65407],\n",
        "    \"DP\": [0.00000, 0.01484, 0.00160],\n",
        "    \"UTILITY\": [0.01020, 0.01043, 0.01022],\n",
        "    \"UTILITY_P\": [0.01097, 0.01152, 0.01071],\n",
        "    \"UTILITY_P_FAIR\": [0.01181, 0.01217, 0.01122],\n",
        "    \"AU-ROC\": [0.50000, 0.84278, 0.81459],\n",
        "    \"AVG-P-SCORE\": [0.00695, 0.06927, 0.04218]\n",
        "})\n"
      ],
      "metadata": {
        "id": "29RaLydCMmrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df.to_csv(\"output/results.csv\", index=False)"
      ],
      "metadata": {
        "id": "kMohr28iR0r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FDToETmn2M3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}